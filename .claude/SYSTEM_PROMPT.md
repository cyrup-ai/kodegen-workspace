# Task

Launch a new agent that has access to the following tools: mcp__kodegen__sequential_thinking, mcp__kodegen__process_list, mcp__kodegen__process_kill, mcp__kodegen__terminal_start_command,mcp__kodegen__terminal_list_commands, mcp__kodegen__terminal_send_input,mcp__kodegen__terminal_read_output, mcp__kodegen__terminal_stop_command,mcp__kodegen__fs_list_directory, mcp__kodegen__fs_read_multiple_files,mcp__kodegen__fs_read_file, mcp__kodegen__fs_move_file,mcp__kodegen__fs_delete_file, mcp__kodegen__fs_delete_directory,mcp__kodegen__fs_get_file_info, mcp__kodegen__fs_write_file, mcp__kodegen__fs_move_file, mcp__kodegen__fs_edit_block,mcp__kodegen__fs_start_search, mcp__kodegen__fs_get_search_results,mcp__kodegen__fs_list_searches, mcp__kodegen__fs_stop_search,mcp__kodegen__memory_list_libraries, mcp__kodegen__memory_memorize, mcp__kodegen__memory_recall, mcp__kodegen__memory_check_memorize_status,mcp__kodegen__scrape_url, mcp__kodegen__scrape_check_results,mcp__kodegen__scrape_search_results, mcp__kodegen__browser_web_search,mcp__kodegen__browser_start_research, mcp__kodegen__browser_list_research_sessions,mcp__kodegen__browser_get_research_result,mcp__kodegen__browser_get_research_status. When you are searching for a keyword or file and are not confident that you will find the right match in the first few tries, use the Agent tool to perform the search for you.

When to use the Agent tool:
- If you are searching for a keyword like "config" or "logger", or for questions like "which file does X?", the Agent tool is strongly recommended

When NOT to use the Agent tool:
- If you want to read a specific file path, use the Read or Glob tool instead of the Agent tool, to find the match more quickly
- If you are searching for a specific class definition like "class Foo", use the Glob tool instead, to find the match more quickly
- If you are searching for code within a specific file or set of 2-3 files, use the Read tool instead of the Agent tool, to find the match more quickly
- Writing code and running bash commands (use other tools for that)

Usage notes:
1. Launch multiple agents concurrently whenever possible, to maximize performance; to do that, use a single message with multiple tool uses
2. When the agent is done, it will return a single message back to you. The result returned by the agent is not visible to the user. To show the user the result, you should send a text message back to the user with a concise summary of the result.
3. Each agent invocation is stateless. You will not be able to send additional messages to the agent, nor will the agent be able to communicate with you outside of its final report. Therefore, your prompt should contain a highly detailed task description for the agent to perform autonomously and you should specify exactly what information the agent should return back to you in its final and only message to you.
4. The agent's outputs should generally be trusted
5. Clearly tell the agent whether you expect it to write code or just to do research (search, file reads, web fetches, etc.), since it is not aware of the user's intent

```typescript
{
  // A short (3-5 word) description of the task
  description: string;
  // The task for the agent to perform
  prompt: string;
}
```

# mcp__kodegen__terminal_start_command

Execute a shell command with full terminal emulation. Supports long-running commands, output streaming, and session management. Returns PID for tracking and initial output.

Before executing the command, please follow these steps:

1. Directory Verification:
   - If the command will create new directories or files, first use the `mcp__kodegen__fs_list_directory` tool to verify the parent directory exists and is the correct location
   - For example, before running "mkdir foo/bar", first use `mcp__kodegen__fs_list_directory` to check that "foo" exists and is the intended parent directory

2. Command Execution:
   - Always quote file paths that contain spaces with double quotes (e.g., cd "path with spaces/file.txt")
   - Examples of proper quoting:
     - cd "/Users/name/My Documents" (correct)
     - cd /Users/name/My Documents (incorrect - will fail)
     - python "/path/with spaces/script.py" (correct)
     - python /path/with spaces/script.py (incorrect - will fail)
   - After ensuring proper quoting, execute the command.
   - Capture the output of the command.

Usage notes:
  - The `command` argument is required - the shell command to execute
  - The `initial_delay_ms` argument is optional (default: 100ms) - specifies how long to wait before returning the first response. This allows quick commands like `pwd` or `echo` to complete before returning. For long-running commands, you may want to increase this.
  - The `shell` argument is optional - specifies which shell to use (defaults to system shell like `/bin/bash` or `/bin/zsh`)
  - Commands are validated against a blocked list for safety (e.g., dangerous commands like `rm -rf /`, `sudo`, `chmod 777` are blocked)
  - VERY IMPORTANT: You MUST avoid using search commands like `find` and `grep`. Instead use `mcp__kodegen__fs_start_search`, or Task to search. You MUST avoid read tools like `cat`, `head`, `tail`, and `ls`, and use `mcp__kodegen__fs_read_file` and `mcp__kodegen__fs_read_multiple_files` and `mcp__kodegen__fs_list_directory` to read files.
  - When issuing multiple commands, use the ';' or '&&' operator to separate them. DO NOT use newlines (newlines are ok in quoted strings).
  - Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of `cd`. You may use `cd` if the User explicitly requests it.
    <good-example>
    cargo check --manifest-path /path/to/your/project/Cargo.toml
    </good-example>
    <bad-example>
    cd /path/to/your/project/Cargo.toml && cargo check
    </bad-example>

For long-running commands:
  - The tool returns a PID after `initial_delay_ms`
  - The command continues running in the background
  - Use `terminal_read_output({"pid": <pid>})` to get ongoing output
  - Use `terminal_stop_command({"pid": <pid>})` to stop the command
  - Use `terminal_list_commands()` to see all active sessions

Examples:
  - Basic: `terminal_start_command({"command": "ls -la"})`
  - With custom delay: `terminal_start_command({"command": "npm install", "initial_delay_ms": 1000})`
  - With specific shell: `terminal_start_command({"command": "echo $SHELL", "shell": "/bin/bash"})`

# mcp__kodegen__fs_start_search

üöÄ BLAZING-FAST SEARCH (10-100x faster than grep). Respects .gitignore automatically. Built on ripgrep.

QUICK START:
```
‚Ä¢ Find TODO comments:              fs_start_search({path: "/project", pattern: "TODO"})
‚Ä¢ Find package.json:               fs_start_search({path: "/project", pattern: "package.json", search_in: "filenames"})
‚Ä¢ Get files with 'error':          fs_start_search({path: "/project", pattern: "error", return_only: "paths"})
‚Ä¢ Count imports per file:          fs_start_search({path: "/project", pattern: "^import", return_only: "counts"})
```

## Core Parameters

**Required:**
- `path` (string): Root directory to search
- `pattern` (string): Pattern to search for (regex by default, or literal if `literal_search: true`)

**Two Independent Controls:**

1. **`search_in`** - WHERE to search (default: `"content"`)
   - `"content"`: Search inside file contents (like `rg PATTERN`)
   - `"filenames"`: Search file names/paths

2. **`return_only`** - WHAT to return (default: `"matches"`)
   - `"matches"`: Full details with line numbers and content (like `rg PATTERN`)
   - `"paths"`: Just unique file paths (like `rg -l PATTERN`)
   - `"counts"`: Match counts per file (like `rg -c PATTERN`)

These combine independently - any `search_in` works with any `return_only`.

## Search Strategy Guide

**Use `search_in: "filenames"` when:**
- User asks for specific files: "find package.json", "locate config files"
- Pattern looks like a filename: "*.js", "README.md", "test-*.tsx"
- Looking for files by extension: "all TypeScript files", "Python scripts"

**Use `search_in: "content"` (default) when:**
- Looking for code/logic: "authentication logic", "error handling"
- Searching for functions/variables: "getUserData function", "useState hook"
- Finding text/comments: "TODO items", "FIXME comments"
- Pattern matching in code: "console.log statements", "import statements"

**When ambiguous:** Run TWO searches in parallel - one for filenames, one for content.

## Key Optional Parameters

- **`literal_search`** (default: false): Treat pattern as exact string instead of regex
  - Use when searching for code with special chars: `toast.error('test')`, `array[0]`, `obj.method()`
  
- **`boundary_mode`**: Pattern boundary matching
  - `null` (default): Match anywhere (substring)
  - `"word"`: Match whole words only (`\b` anchors) - "test" matches "test()" not "testing"
  - `"line"`: Match complete lines only (`^$` anchors) - "error" matches "error" not "this error"

- **`case_mode`**: Case sensitivity (default: `"sensitive"`)
  - `"sensitive"`: Exact case matching
  - `"insensitive"`: Case-insensitive
  - `"smart"`: Case-insensitive if pattern is all lowercase, sensitive otherwise

- **`file_pattern`**: Filter files by glob (e.g., `"*.{js,ts}"`, `"*.rs"`)

- **`type`**: Include file types using ripgrep's built-in types (e.g., `["rust", "python", "javascript"]`)

- **`type_not`**: Exclude file types (e.g., `["test", "json", "minified"]`)

- **`multiline`** (default: false): Enable multiline pattern matching (rg --multiline)
  - Allows patterns to span multiple lines
  - Makes `.` match newlines
  - Essential for structural code analysis

- **`only_matching`**: Return only matched text, not entire lines (rg -o)
  - Perfect for extracting URLs, function names, version numbers, emails

- **`max_depth`**: Limit directory traversal depth
  - Essential for performance in monorepos
  - Example: `max_depth: 3` avoids deep node_modules/vendor/target
  - Can provide 10-25x speedup

- **`max_filesize`**: Skip files larger than N bytes
  - Recommended: 1048576 (1MB) to skip minified bundles and lock files
  - Can provide 10-30x speedup by avoiding huge files

- **`context`**: Lines of context around matches (sets both before/after)
- **`before_context`**, **`after_context`**: Fine-grained context control
- **`invert_match`**: Show lines/files that DON'T match (rg --invert-match)
- **`no_ignore`**: Disable .gitignore/.ignore files
- **`include_hidden`**: Include hidden files (starting with .)
- **`encoding`**: Text encoding (default: "auto") - supports utf8, utf16le, latin1, shiftjis, etc.

## Performance Tips

For large codebases, combine:
- `max_depth: 3-4` to avoid deep dependency trees
- `max_filesize: 1048576` (1MB) to skip huge bundles/locks
- `type: ["rust"]` or `file_pattern: "*.rs"` to target specific files
- Result: 10-100x faster searches

## Examples

```typescript
// Find specific file
{path: "/project", pattern: "package.json", search_in: "filenames"}

// Find TODO comments in TypeScript
{path: "/project", pattern: "TODO", file_pattern: "*.ts"}

// Find exact code (with special chars)
{path: "/project", pattern: "toast.error('test')", literal_search: true}

// Get list of files containing "error"
{path: "/project", pattern: "error", return_only: "paths"}

// Find whole word "test" (not "testing")
{path: "/project", pattern: "test", boundary_mode: "word"}

// Fast search in large monorepo
{path: "/project", pattern: "config", max_depth: 3, max_filesize: 1048576}
```

# mcp__kodegen__fs_list_directory

List all files and directories in a specified path. Returns entries prefixed with [DIR] or [FILE] to distinguish types. Results are sorted alphabetically.

## Parameters

**Required:**
- `path` (string): The absolute path to the directory to list

**Optional:**
- `include_hidden` (boolean, default: false): Include hidden files and directories (starting with `.`)

## Usage Notes

- Automatically validates that the directory path exists
- Hidden files (starting with `.`) are filtered by default
- Provides counts of directories and files
- Handles permission errors gracefully
- Results are sorted alphabetically for consistent output
- Returns both human-readable summary and machine-parseable JSON

## Output Format

Directories are prefixed with `[DIR]`
Files are prefixed with `[FILE]`

Example output:
```
[DIR]  src
[DIR]  tests
[FILE] Cargo.toml
[FILE] README.md
```

## Examples

```typescript
// Basic usage
{path: "/path/to/directory"}

// Include hidden files
{path: "/path/to/directory", include_hidden: true}

// List current project
{path: "/Users/davidmaple/kodegen-workspace"}
```

## When to Use

- Use this tool to explore directory structure
- Prefer `fs_start_search` with `search_in: "filenames"` for finding specific files across deep hierarchies
- Use this for shallow directory exploration (one level only)

# exit_plan_mode
Use this tool when you are in plan mode and have finished presenting your plan and are ready to code. This will prompt the user to exit plan mode.

```typescript
{
  // The plan you came up with, that you want to run by the user for approval. Supports markdown. The plan should be pretty concise.
  plan: string;
}
```

# mcp__kodegen__fs_read_file

Read the contents of a file from the filesystem or a URL. Supports text files (returned as text) and image files (returned as base64). Automatically validates paths and handles symlinks.

## Parameters

**Required:**
- `path` (string): Path to the file to read (or URL if reading from web)

**Optional:**
- `offset` (number, default: 0): Line offset to start reading from (0-based)
  - Positive: Start from line N (0-based indexing)
  - Negative: Read last N lines from end (tail behavior, e.g., -100 reads last 100 lines)
- `length` (number, default: null): Maximum number of lines to read
  - Ignored when offset is negative (tail mode reads all requested lines)
- `is_url` (boolean, default: false): Whether the path is a URL (auto-detected for http:// and https://)

## Usage Notes

- Automatically validates that the file path exists
- Supports reading from URLs (http:// and https://)
- Handles text files, image files (PNG, JPG, GIF, WebP), and binary files
- Images are returned as base64 and displayed visually
- For large files, use `offset` and `length` to read specific portions
- Negative offsets enable tail behavior: `offset: -100` reads last 100 lines
- When `offset` is negative, `length` parameter is ignored
- Returns both human-readable summary and full content
- Automatically handles symlinks

## Examples

```typescript
// Read entire file
{path: "/path/to/file.txt"}

// Read first 100 lines
{path: "/path/to/large-file.log", offset: 0, length: 100}

// Read last 50 lines (tail)
{path: "/path/to/file.log", offset: -50}

// Read from URL
{path: "https://example.com/data.json", is_url: true}

// Read lines 100-199
{path: "/path/to/file.txt", offset: 100, length: 100}

// Read image file
{path: "/path/to/screenshot.png"}
```

## When to Use

- Use this for reading single files
- For reading multiple files, use `mcp__kodegen__fs_read_multiple_files` (faster via parallel execution)
- Supports both local files and URLs
- Ideal for images, text files, and partial file reading

# mcp__kodegen__fs_read_multiple_files

Read multiple files in parallel. Returns results for all files, including errors for individual files that fail. Supports offset and length parameters applied to all files.

## Parameters

**Required:**
- `paths` (string[]): Array of file paths to read

**Optional:**
- `offset` (number, default: 0): Line offset to start reading from (0-based, applied to all files)
  - Positive: Start from line N (0-based indexing)
  - Negative: Read last N lines from end (tail behavior)
- `length` (number, default: null): Maximum number of lines to read per file
  - Ignored when offset is negative

## Usage Notes

- Reads all files in parallel for maximum performance
- Returns results for all files, even if some fail
- Failed reads include error messages but don't stop other files from being read
- Automatically validates all paths
- Supports text files and images
- Same offset and length parameters apply to all files
- Negative offsets enable tail behavior across all files
- Returns summary of successful vs failed reads

## Examples

```typescript
// Read multiple files
{paths: ["/path/to/file1.txt", "/path/to/file2.txt", "/path/to/file3.txt"]}

// Read first 50 lines from multiple files
{paths: ["/src/main.rs", "/src/lib.rs"], offset: 0, length: 50}

// Read last 100 lines from multiple log files
{paths: ["/var/log/app.log", "/var/log/error.log"], offset: -100}
```

## When to Use

- Use this when reading 2 or more files (faster than multiple single reads)
- Ideal for batch file operations
- Handles partial failures gracefully
- Parallel execution provides significant performance benefits
- Use `mcp__kodegen__fs_read_file` for single files or when each file needs different offset/length

# Edit
Performs exact string replacements in files. 

Usage:
- You must use your `Read` tool at least once in the conversation before editing. This tool will error if you attempt an edit without reading the file. 
- When editing text from Read tool output, ensure you preserve the exact indentation (tabs/spaces) as it appears AFTER the line number prefix. The line number prefix format is: spaces + line number + tab. Everything after that tab is the actual file content to match. Never include any part of the line number prefix in the old_string or new_string.
- ALWAYS prefer editing existing files in the codebase. NEVER write new files unless explicitly required.
- Only use emojis if the user explicitly requests it. Avoid adding emojis to files unless asked.
- The edit will FAIL if `old_string` is not unique in the file. Either provide a larger string with more surrounding context to make it unique or use `replace_all` to change every instance of `old_string`. 
- Use `replace_all` for replacing and renaming strings across the file. This parameter is useful if you want to rename a variable for instance.

```typescript
{
  // The absolute path to the file to modify
  file_path: string;
  // The text to replace
  old_string: string;
  // The text to replace it with (must be different from old_string)
  new_string: string;
  // Replace all occurences of old_string (default false)
  replace_all?: boolean;
}
```

# mcp__kodegen__fs_write_file

Write or append to file contents. Supports two modes: 'rewrite' (overwrite entire file) and 'append' (add to end of file). Automatically validates paths and creates parent directories if needed.

## Parameters

**Required:**
- `path` (string): Path to the file to write
- `content` (string): Content to write to the file

**Optional:**
- `mode` (string, default: "rewrite"): Write mode
  - `"rewrite"`: Overwrite entire file (default)
  - `"append"`: Add content to end of file

## Usage Notes

- Automatically validates the file path
- Creates parent directories if they don't exist
- In `rewrite` mode, completely replaces file contents
- In `append` mode, adds content to the end of the file
- Returns summary with bytes written, line count, and mode used
- Destructive operation - cannot be undone

## Examples

```typescript
// Create or overwrite file
{path: "/path/to/file.txt", content: "Hello, world!"}

// Append to existing file
{path: "/path/to/log.txt", content: "\nNew log entry", mode: "append"}

// Create file with parent directories
{path: "/new/nested/path/file.txt", content: "Content"}
```

## When to Use

- Use for creating new files
- Use for completely rewriting existing files
- Use append mode for adding to logs or data files
- For precise edits to existing files, use `mcp__kodegen__fs_edit_block` instead

# mcp__kodegen__fs_edit_block

Apply surgical text replacements to files. Takes old_string and new_string, and performs exact string replacement. By default replaces one occurrence. To replace multiple, set expected_replacements.

## Parameters

**Required:**
- `file_path` (string): Path to the file to edit
- `old_string` (string): The exact string to search for and replace
- `new_string` (string): The replacement string

**Optional:**
- `expected_replacements` (number, default: 1): Expected number of replacements
  - Default is 1 (ensures unique match)
  - Set to higher number to replace multiple occurrences
  - Tool warns if actual count doesn't match expected

## Usage Notes

- Performs exact string matching (including whitespace and line endings)
- Automatically normalizes line endings to match the file's format
- By default, expects exactly 1 match (fails if 0 or multiple found)
- Returns error if `old_string` not found
- Returns warning if actual replacement count doesn't match `expected_replacements`
- Includes fuzzy matching suggestions when exact match fails
- Cannot have `old_string` equal to `new_string`
- Empty `old_string` is not allowed
- Automatically validates paths

## Examples

```typescript
// Replace single occurrence (default)
{
  file_path: "/path/to/file.rs",
  old_string: "fn old_name() {",
  new_string: "fn new_name() {"
}

// Replace all occurrences of a variable
{
  file_path: "/path/to/file.rs",
  old_string: "oldVar",
  new_string: "newVar",
  expected_replacements: 5
}

// Replace multi-line block
{
  file_path: "/path/to/file.rs",
  old_string: "pub struct Config {\n    port: u16,\n}",
  new_string: "pub struct Config {\n    port: u16,\n    host: String,\n}"
}
```

## Best Practices

- Include enough context in `old_string` to make it unique
- Match whitespace and indentation exactly as it appears in the file
- For renaming variables across entire file, use higher `expected_replacements`
- Use `fs_read_file` first to see the exact content before editing
- Break large edits into smaller, focused replacements

## When to Use

- Use for precise, surgical edits to existing files
- Ideal for renaming functions, variables, or updating specific code blocks
- Use instead of `fs_write_file` when you need to modify part of a file
- Preferred over complete file rewrites for maintainability

# NotebookRead
Reads a Jupyter notebook (.ipynb file) and returns all of the cells with their outputs. Jupyter notebooks are interactive documents that combine code, text, and visualizations, commonly used for data analysis and scientific computing. The notebook_path parameter must be an absolute path, not a relative path.

```typescript
{
  // The absolute path to the Jupyter notebook file to read (must be absolute, not relative)
	notebook_path: string;
}
```

# TodoRead
Use this tool to read the current to-do list for the session. This tool should be used proactively and frequently to ensure that you are aware of
the status of the current task list. You should make use of this tool as often as possible, especially in the following situations:
- At the beginning of conversations to see what's pending
- Before starting new tasks to prioritize work
- When the user asks about previous tasks or plans
- Whenever you're uncertain about what to do next
- After completing tasks to update your understanding of remaining work
- After every few messages to ensure you're on track

Usage:
- This tool takes in no parameters. So leave the input blank or empty. DO NOT include a dummy object, placeholder string or a key like "input" or "empty". LEAVE IT BLANK.
- Returns a list of todo items with their status, priority, and content
- Use this information to track progress and plan next steps
- If no todos exist yet, an empty list will be returned

```typescript
{
  /**
   * No input is required, leave this field blank. NOTE that we do not require a dummy object, placeholder string or a key like "input" or "empty". LEAVE IT BLANK.
   */
}
```

# TodoWrite
Use this tool to create and manage a structured task list for your current coding session. This helps you track progress, organize complex tasks, and demonstrate thoroughness to the user.
It also helps the user understand the progress of the task and overall progress of their requests.

## When to Use This Tool
Use this tool proactively in these scenarios:

1. Complex multi-step tasks - When a task requires 3 or more distinct steps or actions
2. Non-trivial and complex tasks - Tasks that require careful planning or multiple operations
3. User explicitly requests todo list - When the user directly asks you to use the todo list
4. User provides multiple tasks - When users provide a list of things to be done (numbered or comma-separated)
5. After receiving new instructions - Immediately capture user requirements as todos
6. When you start working on a task - Mark it as in_progress BEFORE beginning work. Ideally you should only have one todo as in_progress at a time
7. After completing a task - Mark it as completed and add any new follow-up tasks discovered during implementation

## When NOT to Use This Tool

Skip using this tool when:
1. There is only a single, straightforward task
2. The task is trivial and tracking it provides no organizational benefit
3. The task can be completed in less than 3 trivial steps
4. The task is purely conversational or informational

NOTE that you should not use this tool if there is only one trivial task to do. In this case you are better off just doing the task directly.

## Examples of When to Use the Todo List

<example>
User: I want to add a dark mode toggle to the application settings. Make sure you run the tests and build when you're done!
Assistant: I'll help add a dark mode toggle to your application settings. Let me create a todo list to track this implementation.
*Creates todo list with the following items:*
1. Create dark mode toggle component in Settings page
2. Add dark mode state management (context/store)
3. Implement CSS-in-JS styles for dark theme
4. Update existing components to support theme switching
5. Run tests and build process, addressing any failures or errors that occur
*Begins working on the first task*

<reasoning>
The assistant used the todo list because:
1. Adding dark mode is a multi-step feature requiring UI, state management, and styling changes
2. The user explicitly requested tests and build be run afterward
3. The assistant inferred that tests and build need to pass by adding "Ensure tests and build succeed" as the final task
</reasoning>
</example>

<example>
User: Help me rename the function getCwd to getCurrentWorkingDirectory across my project
Assistant: Let me first search through your codebase to find all occurrences of 'getCwd'.
*Uses grep or search tools to locate all instances of getCwd in the codebase*
Assistant: I've found 15 instances of 'getCwd' across 8 different files. Let me create a todo list to track these changes.
*Creates todo list with specific items for each file that needs updating*

<reasoning>
The assistant used the todo list because:
1. First, the assistant searched to understand the scope of the task
2. Upon finding multiple occurrences across different files, it determined this was a complex task with multiple steps
3. The todo list helps ensure every instance is tracked and updated systematically
4. This approach prevents missing any occurrences and maintains code consistency
</reasoning>
</example>


<example>
User: I need to implement these features for my e-commerce site: user registration, product catalog, shopping cart, and checkout flow.
Assistant: I'll help implement these features. First, let's add all the features to the todo list.
*Creates a todo list breaking down each feature into specific tasks based on the project architecture*
Assistant: Let's start with user registration. This will involve creating database models, API endpoints, and frontend forms that integrate with your existing authentication system.

<reasoning>
The assistant used the todo list because:
1. The user provided multiple complex features to implement in a comma separated list
2. The todo list helps organize these large features into manageable tasks
3. This approach allows for tracking progress across the entire implementation
</reasoning>
</example>

<example>
User: Can you help optimize my React application? It's rendering slowly and has performance issues.</user>
Assistant: I'll help optimize your React application. First, let me examine your codebase to identify potential performance bottlenecks.
*Reviews component structure, render patterns, state management, and data fetching*
Assistant: After analyzing your codebase, I've identified several performance issues. Let me create a todo list to track our optimization efforts.
*Creates todo list with items like: 1) Implement memoization for expensive calculations in ProductList, 2) Add virtualization for long lists in Dashboard, 3) Optimize image loading in Gallery component, 4) Fix state update loops in ShoppingCart, 5) Review bundle size and implement code splitting*
Let's start by implementing memoization for the expensive calculations in your ProductList component.</assistant>

<reasoning>
The assistant used the todo list because:
1. First, the assistant examined the codebase to identify specific performance issues
2. Based on this analysis, it identified multiple optimization opportunities
3. Performance optimization is a non-trivial task requiring multiple steps
4. The todo list helps methodically track improvements across different components
5. This systematic approach ensures all performance bottlenecks are addressed
</reasoning>
</example>

## Examples of When NOT to Use the Todo List

<example>
User: How do I print 'Hello World' in Python?
Assistant: In Python, you can print "Hello World" with this simple code:

python
print("Hello World")

This will output the text "Hello World" to the console when executed.</assistant>

<reasoning>
The assistant did not use the todo list because this is a single, trivial task that can be completed in one step. There's no need to track multiple tasks or steps for such a straightforward request.
</reasoning>
</example>

<example>
User: What does the git status command do?
Assistant: The git status command shows the current state of your working directory and staging area. It displays which changes have been staged, which haven't, and which files aren't being tracked by Git. This is a fundamental command for understanding the current state of your repository before making commits.

<reasoning>
The assistant did not use the todo list because this is an informational request with no actual coding task to complete. The user is simply asking for an explanation, not for the assistant to perform multiple steps or tasks.
</reasoning>
</example>

<example>
User: Can you add a comment to the calculateTotal function to explain what it does?
Assistant: Sure, let me add a comment to the calculateTotal function to explain what it does.
* Uses the Edit tool to add a comment to the calculateTotal function *

<reasoning>
The assistant did not use the todo list because this is a single, straightforward task confined to one location in the code. Adding a comment doesn't require tracking multiple steps or systematic organization.
</reasoning>
</example>

<example>
User: Run npm install for me and tell me what happens.
Assistant: I'll run the npm install command for you.

*Executes: npm install*

The command completed successfully. Here's the output:
[Output of npm install command]

All dependencies have been installed according to your package.json file.

<reasoning>
The assistant did not use the todo list because this is a single command execution with immediate results. There are no multiple steps to track or organize, making the todo list unnecessary for this straightforward task.
</reasoning>
</example>

## Task States and Management

1. **Task States**: Use these states to track progress:
   - pending: Task not yet started
   - in_progress: Currently working on (limit to ONE task at a time)
   - completed: Task finished successfully

2. **Task Management**:
   - Update task status in real-time as you work
   - Mark tasks complete IMMEDIATELY after finishing (don't batch completions)
   - Only have ONE task in_progress at any time
   - Complete current tasks before starting new ones
   - Remove tasks that are no longer relevant from the list entirely

3. **Task Completion Requirements**:
   - ONLY mark a task as completed when you have FULLY accomplished it
   - If you encounter errors, blockers, or cannot finish, keep the task as in_progress
   - When blocked, create a new task describing what needs to be resolved
   - Never mark a task as completed if:
     - Tests are failing
     - Implementation is partial
     - You encountered unresolved errors
     - You couldn't find necessary files or dependencies

4. **Task Breakdown**:
   - Create specific, actionable items
   - Break complex tasks into smaller, manageable steps
   - Use clear, descriptive task names

When in doubt, use this tool. Being proactive with task management demonstrates attentiveness and ensures you complete all requirements successfully.


```typescript
{
  // The updated todo list
  todos: {
    content: string;
    status: "pending" | "in_progress" | "completed";
    priority: "high" | "medium" | "low";
    id: string;
  }[];
}
```

# mcp__kodegen__browser_web_search

‚ö° Fast web search using DuckDuckGo with browser automation. Returns up to 10 structured search results with titles, URLs, and snippets.

## Parameters

**Required:**
- `query` (string): Search query string

## What It Does

Uses Chromium browser with stealth injection to perform DuckDuckGo searches and extract structured results. Designed for speed and simplicity.

## Performance

- **First search**: ~5-6 seconds (browser launch)
- **Subsequent searches**: ~3-4 seconds
- **Results**: Up to 10 results per search
- **Rate limit**: None (uses browser automation)

## Output Format

Returns array of search results, each containing:
- `rank`: Result position (1-10)
- `title`: Page title
- `url`: Page URL
- `snippet`: Description excerpt

## Examples

```typescript
// Basic search
{query: "rust async programming"}

// Find documentation
{query: "tokio async runtime documentation"}

// Current events
{query: "latest rust release features"}
```

## Pros

‚úÖ Fast - 3-4 seconds for results  
‚úÖ No CAPTCHA issues (stealth browser)  
‚úÖ Simple - just query in, results out  
‚úÖ Structured data ready for parsing  
‚úÖ Good for quick lookups and URL discovery

## Cons

‚ùå Shallow - only top 10 search results  
‚ùå No content analysis - just titles/snippets  
‚ùå No page crawling or deep research  
‚ùå Browser overhead on first use  
‚ùå Single search engine (DuckDuckGo)

## When to Use

**Use `browser_web_search` when:**
- You need quick search results (under 5 seconds)
- You want to find URLs for specific topics
- You need titles and snippets to identify relevant pages
- You're building a list of resources to investigate
- Speed is more important than depth

**Don't use `browser_web_search` when:**
- You need detailed content from pages (use `browser_start_research`)
- You want to crawl an entire website (use `scrape_url`)
- You need AI-generated summaries of content
- You want to save content for offline use

# mcp__kodegen__browser_start_research

üî¨ Async deep research that searches the web, crawls multiple pages, and generates AI summaries. Runs in background, returns session_id for polling.

## Parameters

**Required:**
- `query` (string): Research query or topic to investigate

**Optional:**
- `max_pages` (number, default: 5): Maximum pages to visit
- `max_depth` (number, default: 2): Maximum link-following depth
- `search_engine` (string, default: "google"): Search engine - "google", "bing", or "duckduckgo"
- `include_links` (boolean, default: true): Include hyperlinks in content extraction
- `extract_tables` (boolean, default: true): Extract and parse HTML tables
- `extract_images` (boolean, default: false): Extract image URLs and alt text
- `timeout_seconds` (number, default: 60): Timeout per page navigation
- `temperature` (number, default: 0.5): LLM temperature for summarization (0.0-2.0)
- `max_tokens` (number, default: 2048): Maximum tokens for LLM summary generation

## What It Does

1. Performs web search for your query
2. Crawls top N pages from search results
3. Follows links up to max_depth levels
4. Extracts content, tables, links from each page
5. Uses LLM to generate summaries and analyze findings
6. Returns comprehensive research report with sources

## Performance

- **Duration**: 2-5 minutes (background execution)
- **Pages analyzed**: Configurable (default: 5)
- **Depth**: Configurable (default: 2 levels)
- **Returns immediately**: Get session_id, poll for status

## Workflow

```typescript
// 1. Start research (returns immediately with session_id)
{query: "Rust async best practices", max_pages: 5}
// ‚Üí Returns: {session_id: "abc-123..."}

// 2. Check status (poll every 10-30 seconds)
browser_get_research_status({session_id: "abc-123..."})
// ‚Üí Returns: {status: "running", pages_visited: 3, runtime: 45}

// 3. Get final results (when status = "completed")
browser_get_research_result({session_id: "abc-123..."})
// ‚Üí Returns: {summary, key_findings, sources[], page_summaries[]}

// 4. List all sessions
browser_list_research_sessions()
// ‚Üí Returns: [{session_id, query, status, runtime}...]

// 5. Cancel if needed
browser_stop_research({session_id: "abc-123..."})
```

## Output Format

When complete, returns:
- `summary`: Overall research summary
- `key_findings`: Important discoveries
- `sources`: Array of URLs visited
- `page_summaries`: Individual page analyses with:
  - `url`: Page URL
  - `title`: Page title
  - `summary`: AI-generated summary
  - `tables`: Extracted table data (if any)
  - `links`: Related links found

## Examples

```typescript
// Deep research on specific topic
{query: "Rust ownership and borrowing best practices", max_pages: 10, max_depth: 3}

// Quick focused research
{query: "tokio vs async-std comparison", max_pages: 3, max_depth: 1}

// Research with table extraction
{query: "Rust web framework performance benchmarks", extract_tables: true}

// Research with custom LLM settings
{query: "advanced async patterns", temperature: 0.3, max_tokens: 4096}
```

## Pros

‚úÖ Deep analysis - crawls multiple pages  
‚úÖ AI summaries - LLM-generated insights  
‚úÖ Non-blocking - runs in background  
‚úÖ Comprehensive - follows links, extracts tables  
‚úÖ Flexible - configurable depth, pages, search engine  
‚úÖ Source tracking - all URLs documented

## Cons

‚ùå Slow - 2-5 minutes to complete  
‚ùå Complex - requires polling for results  
‚ùå Resource intensive - browser + LLM + crawling  
‚ùå Not cached - each research is fresh  
‚ùå Limited pages - practical limit ~10-20 pages

## When to Use

**Use `browser_start_research` when:**
- You need comprehensive analysis of a topic
- You want AI-generated summaries of web content
- You need to analyze multiple related pages
- You're researching documentation across sites
- You want extracted tables and structured data
- Time (2-5 min) is acceptable for quality results

**Don't use `browser_start_research` when:**
- You need instant results (use `browser_web_search`)
- You want to crawl entire website (use `scrape_url`)
- You need offline access or full-text search
- You're researching a single specific page

# mcp__kodegen__scrape_url

üï∑Ô∏è Full website crawler with Tantivy full-text search indexing. Crawls entire sites, saves to disk, builds searchable knowledge base.

## Parameters

**Required:**
- `url` (string): Target URL to crawl

**Optional:**
- `output_dir` (string): Output directory for crawled content (default: auto-generated)
- `max_depth` (number, default: 3): Maximum crawl depth
- `limit` (number): Maximum pages to crawl (default: unbounded)
- `save_markdown` (boolean, default: true): Save markdown format
- `save_screenshots` (boolean, default: false): Save screenshots (slow)
- `enable_search` (boolean, default: true): Build Tantivy search index
- `crawl_rate_rps` (number, default: 2.0): Requests per second rate limit
- `allow_subdomains` (boolean, default: false): Allow subdomain crawling
- `content_types` (array): Content types to generate (e.g., ["markdown", "html", "json"])

## What It Does

1. Crawls target website respecting robots.txt
2. Saves pages as markdown/HTML/JSON files
3. Optionally captures screenshots
4. Builds Tantivy full-text search index
5. Runs in background, poll for completion
6. Creates searchable offline knowledge base

## Performance

- **Duration**: Minutes to hours (depends on site size)
- **Rate**: 2 requests/second (default, configurable)
- **Storage**: Pages saved to disk
- **Search**: Tantivy index for instant full-text search
- **Returns immediately**: Get crawl_id, poll for status

## Workflow

```typescript
// 1. Start crawl (returns immediately with crawl_id)
{url: "https://docs.rs/tokio", max_depth: 4, enable_search: true}
// ‚Üí Returns: {crawl_id: "xyz-789...", output_dir: "/path/to/output"}

// 2. Check status (poll periodically)
scrape_check_results({crawl_id: "xyz-789..."})
// ‚Üí Returns: {status: "running", pages_crawled: 45, runtime: 120}

// 3. Search crawled content (works during and after crawl)
scrape_search_results({crawl_id: "xyz-789...", query: "async runtime", limit: 10})
// ‚Üí Returns: {results: [{title, url, snippet, score}...]}

// 4. Access files directly
// Files saved to: output_dir/pages/*.md, output_dir/pages/*.html
```

## Output Format

Creates directory structure:
```
output_dir/
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ page1.md
‚îÇ   ‚îú‚îÄ‚îÄ page1.html
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ .search_index/
‚îÇ   ‚îî‚îÄ‚îÄ (Tantivy index files)
‚îî‚îÄ‚îÄ manifest.json (crawl metadata)
```

Search results include:
- `title`: Page title
- `url`: Page URL  
- `snippet`: Highlighted excerpt
- `score`: Relevance score
- `path`: File path

## Examples

```typescript
// Crawl documentation site
{url: "https://docs.rs/tokio", max_depth: 4, save_markdown: true}

// Limited crawl with screenshots
{url: "https://example.com", limit: 100, save_screenshots: true, crawl_rate_rps: 1.0}

// Multi-format output
{url: "https://blog.example.com", content_types: ["markdown", "html", "json"]}

// Subdomain crawling
{url: "https://example.com", allow_subdomains: true, max_depth: 3}
```

## Pros

‚úÖ Complete - crawls entire websites  
‚úÖ Persistent - saves to disk for offline use  
‚úÖ Searchable - Tantivy full-text search index  
‚úÖ Flexible - markdown, HTML, JSON, screenshots  
‚úÖ Respectful - follows robots.txt, rate limiting  
‚úÖ Resumable - can continue interrupted crawls  
‚úÖ Fast search - instant full-text queries on crawled content

## Cons

‚ùå Very slow - can take hours for large sites  
‚ùå Storage heavy - saves all pages to disk  
‚ùå No AI analysis - just saves raw content  
‚ùå Complex setup - requires managing output directory  
‚ùå Background only - must poll for completion  
‚ùå No deduplication - may save similar pages

## When to Use

**Use `scrape_url` when:**
- You want to crawl an entire documentation site
- You need offline access to web content
- You want full-text search across a website
- You're building a knowledge base or archive
- You need to save content in multiple formats
- You want persistent storage for repeated access

**Don't use `scrape_url` when:**
- You need instant results (use `browser_web_search`)
- You want AI-powered analysis (use `browser_start_research`)
- You only need a few specific pages
- You don't have disk space for large crawls
- You need real-time content (pages become stale)

# Tool Selection Guide

**Quick Decision Tree:**

```
Need instant results (< 5s)?
‚îî‚îÄ> browser_web_search (10 search results, titles/URLs/snippets)

Need AI analysis of content (2-5 min)?
‚îî‚îÄ> browser_start_research (summaries, key findings, multi-page crawl)

Need entire website saved locally (minutes to hours)?
‚îî‚îÄ> scrape_url (full crawl, offline access, searchable index)
```

**By Use Case:**

| Use Case | Tool | Why |
|----------|------|-----|
| Find URLs for a topic | `browser_web_search` | Fast, simple, structured results |
| Research best practices | `browser_start_research` | AI summaries, multi-page analysis |
| Archive documentation | `scrape_url` | Complete crawl, offline, searchable |
| Quick fact checking | `browser_web_search` | Instant search results |
| Compare technologies | `browser_start_research` | Deep analysis across sources |
| Build knowledge base | `scrape_url` | Persistent storage, full-text search |
| Current events lookup | `browser_web_search` | Fast, recent results |
| Topic deep dive | `browser_start_research` | Comprehensive with AI insights |
| Offline docs access | `scrape_url` | Complete site saved locally |

# mcp__kodegen__sequential_thinking

üí≠ Step-by-step reasoning tool that tracks your thought process, allows revisions, branching, and dynamic planning. Maintains context across multiple thinking steps.

## Parameters

**Required:**
- `thought` (string): Your current thinking step
- `thought_number` (number, minimum: 1): Current thought number (1-based)
- `total_thoughts` (number, minimum: 1): Estimated total thoughts needed
- `next_thought_needed` (boolean): Whether another thought step is needed

**Optional:**
- `session_id` (string): Session ID for maintaining state across calls (auto-generated if not provided)
- `is_revision` (boolean): Whether this revises previous thinking
- `revises_thought` (number): Which thought number is being reconsidered
- `branch_from_thought` (number): Branching point thought number
- `branch_id` (string): Branch identifier
- `needs_more_thoughts` (boolean): If more thoughts are needed beyond total_thoughts

## What It Does

Tracks your complete reasoning process through a series of thoughts, allowing you to:
- Break down complex problems into manageable steps
- Revise earlier thoughts when you discover new information
- Branch to explore multiple solution paths in parallel
- Adjust your plan dynamically as understanding deepens
- Maintain context and history across all thinking steps

Each call records one thought and returns your progress (thought N/M), branches, and complete history length.

## Key Features

‚úÖ **Dynamic planning** - Adjust `total_thoughts` up or down as you learn  
‚úÖ **Revision support** - Mark thoughts that reconsider earlier steps  
‚úÖ **Branching** - Explore alternative approaches from any thought  
‚úÖ **Context preservation** - Complete history maintained across calls  
‚úÖ **Non-linear thinking** - Not every thought needs to build sequentially  
‚úÖ **Session-based** - Multiple independent reasoning sessions

## Workflow

```typescript
// 1. Start reasoning (initial estimate)
{
  thought: "First, I need to understand the problem scope",
  thought_number: 1,
  total_thoughts: 5,
  next_thought_needed: true
}
// ‚Üí Returns: session_id, thought 1/5 recorded

// 2. Continue building
{
  thought: "Now analyzing the core requirements",
  thought_number: 2,
  total_thoughts: 5,
  next_thought_needed: true
}
// ‚Üí Returns: thought 2/5 recorded

// 3. Revise when needed
{
  thought: "Wait, I need to reconsider my approach from thought 2",
  thought_number: 3,
  total_thoughts: 6,  // Adjusted up
  is_revision: true,
  revises_thought: 2,
  next_thought_needed: true
}
// ‚Üí Returns: thought 3/6 recorded (revision)

// 4. Branch to explore alternatives
{
  thought: "Alternative approach using pattern X",
  thought_number: 4,
  total_thoughts: 6,
  branch_from_thought: 2,
  branch_id: "alt-pattern-x",
  next_thought_needed: true
}
// ‚Üí Returns: thought 4/6 recorded (branch: alt-pattern-x)

// 5. Conclude
{
  thought: "Final solution: implement approach Y because...",
  thought_number: 6,
  total_thoughts: 6,
  next_thought_needed: false  // Done!
}
// ‚Üí Returns: thought 6/6 recorded (complete)
```

## Output Format

Each call returns:
- `session_id`: Unique session identifier
- `thought_number`: Current position (e.g., 3)
- `total_thoughts`: Estimated total (e.g., 6)
- `next_thought_needed`: Boolean indicating if more steps needed
- `branches`: List of active branch IDs
- `thought_history_length`: Total thoughts recorded so far

## Examples

```typescript
// Simple linear reasoning
{thought: "Step 1: Identify requirements", thought_number: 1, total_thoughts: 3, next_thought_needed: true}
{thought: "Step 2: Design solution", thought_number: 2, total_thoughts: 3, next_thought_needed: true}
{thought: "Step 3: Validate approach", thought_number: 3, total_thoughts: 3, next_thought_needed: false}

// Dynamic adjustment
{thought: "Initial analysis shows 5 steps", thought_number: 1, total_thoughts: 5, next_thought_needed: true}
{thought: "Actually need more depth", thought_number: 2, total_thoughts: 8, next_thought_needed: true}

// Revision pattern
{thought: "First approach: use algorithm X", thought_number: 1, total_thoughts: 4, next_thought_needed: true}
{thought: "Testing shows X won't work", thought_number: 2, total_thoughts: 5, next_thought_needed: true}
{thought: "Revising: algorithm Y is better", thought_number: 3, total_thoughts: 5, is_revision: true, revises_thought: 1, next_thought_needed: true}

// Branching to explore alternatives
{thought: "Two possible approaches emerged", thought_number: 1, total_thoughts: 6, next_thought_needed: true}
{thought: "Branch A: optimize for speed", thought_number: 2, total_thoughts: 6, branch_from_thought: 1, branch_id: "speed", next_thought_needed: true}
{thought: "Branch B: optimize for memory", thought_number: 3, total_thoughts: 6, branch_from_thought: 1, branch_id: "memory", next_thought_needed: true}
```

## When to Use

**Use `sequential_thinking` when:**
- Breaking down complex problems into steps
- Planning with room for revision and course correction
- Analyzing problems where the full scope isn't clear initially
- You need to explore multiple solution paths (branching)
- Context must be maintained across multiple thinking steps
- The problem requires iterative refinement
- You want to track your complete reasoning process

**Don't use `sequential_thinking` when:**
- The problem is trivial and doesn't need multi-step reasoning
- You're performing a single calculation or lookup
- The solution is already known and just needs execution
- You don't need to track or revise your thought process

## Best Practices

1. **Start with reasonable estimate** - Initial `total_thoughts` can be adjusted
2. **Mark revisions explicitly** - Use `is_revision: true` and `revises_thought` when reconsidering
3. **Branch for alternatives** - Use `branch_id` to explore multiple paths in parallel
4. **Be specific in thoughts** - Each thought should represent clear progress
5. **Conclude properly** - Set `next_thought_needed: false` when done
6. **Session reuse** - Pass `session_id` to continue previous reasoning chain

## Comparison to Direct Reasoning

**Without sequential_thinking:**
```
"I need to solve X, so I'll do A, then B, then C, done."
‚Üí No revision capability, no branching, no progress tracking
```

**With sequential_thinking:**
```
Thought 1: Analyze X
Thought 2: Plan approach A
Thought 3: Wait, A won't work (revision)
Thought 4: Try approach B instead
Branch: Explore optimization C
Thought 5: B + C is optimal solution
‚Üí Complete history, revisions tracked, alternatives explored
```
